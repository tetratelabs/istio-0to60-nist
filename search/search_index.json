{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>Welcome to the Istio 0 to 60 workshop!</p> <p>On this site you will find the hands-on labs for the workshop.</p> <p>In the first lab, we walk you through accessing and configuring your lab environment.</p> <p>Let's begin.</p>"},{"location":"dashboards/","title":"Observability","text":"<p>This lab explores one of the main strengths of Istio: observability.</p> <p>The services in our mesh are automatically observable, without adding any burden on developers.</p>"},{"location":"dashboards/#deploy-the-addons","title":"Deploy the Addons","text":"<p>The Istio distribution provides addons for a number of systems that together provide observability for the service mesh:</p> <ul> <li>Zipkin or Jaeger for distributed tracing</li> <li>Prometheus for metrics collection</li> <li>Grafana provides dashboards for monitoring, using Prometheus as the data source</li> <li>Kiali allows us to visualize the mesh</li> </ul> <p>These addons are located in the <code>samples/addons/</code> folder of the distribution.</p> <ol> <li> <p>Navigate to the addons directory</p> <pre><code>cd ~/istio-1.17.2/samples/addons\n</code></pre> </li> <li> <p>Deploy each addon:</p> <pre><code>kubectl apply -f extras/zipkin.yaml\n</code></pre> <pre><code>kubectl apply -f prometheus.yaml\n</code></pre> <pre><code>kubectl apply -f grafana.yaml\n</code></pre> <pre><code>kubectl apply -f kiali.yaml\n</code></pre> </li> <li> <p>Verify that the <code>istio-system</code> namespace is now running additional workloads for each of the addons.</p> <pre><code>kubectl get pod -n istio-system\n</code></pre> </li> </ol> <p>The <code>istioctl</code> CLI provides convenience commands for accessing the web UIs for each dashboard.</p> <p>Take a moment to review the help information for the <code>istioctl dashboard</code> command:</p> <pre><code>istioctl dashboard --help\n</code></pre>"},{"location":"dashboards/#generate-a-load","title":"Generate a load","text":"<p>In order to have something to observe, we need to generate a load on our system.</p>"},{"location":"dashboards/#install-a-load-generator","title":"Install a load generator","text":"<p>Install a simple load generating tool named <code>siege</code>.</p> <p>We normally install <code>siege</code> with the <code>apt-get</code> package manager. However, given the cloud shell's ephemeral nature, anything installed outside our home directory will vanish after a session timeout.</p> <p>Alternatives:</p> <ol> <li>Install from source. It's a little more work, but does not exhibit the above-mentioned problem.</li> <li>Run the load generator from your laptop.  On a mac, using homebrew the command is <code>brew install siege</code>.</li> </ol> <p>Here are the steps to install from source:</p> <ol> <li> <p>Fetch the package</p> <pre><code>wget http://download.joedog.org/siege/siege-latest.tar.gz\n</code></pre> </li> <li> <p>Unpack it</p> <pre><code>tar -xzf siege-latest.tar.gz\n</code></pre> </li> <li> <p>Navigate into the siege subdirectory with <code>cd siege</code>Tab</p> </li> <li> <p>Run the <code>configure</code> script, and request that siege get installed inside your home directory</p> <pre><code>./configure --prefix=$HOME\n</code></pre> </li> <li> <p>Build the code</p> <pre><code>make\n</code></pre> </li> <li> <p>Finally, install (copies the binary to <code>~/bin</code>)</p> <pre><code>make install\n</code></pre> </li> </ol> <p>Feel free to delete (or preserve) the downloaded tar file and source code.</p>"},{"location":"dashboards/#generate-a-load_1","title":"Generate a load","text":"<p>With <code>siege</code> now installed, familiarize yourself with the command and its options.</p> <pre><code>siege --help\n</code></pre> <p>Run the following command to generate a mild load against the application.</p> <pre><code>siege --delay=3 --concurrent=3 --time=20M http://$GATEWAY_IP/\n</code></pre> <p>Note</p> <p>The <code>siege</code> command stays in the foreground while it runs. It may be simplest to leave it running, and open a separate terminal in your cloud shell environment.</p>"},{"location":"dashboards/#kiali","title":"Kiali","text":"<p>Launch the Kiali dashboard:</p> <pre><code>istioctl dashboard kiali\n</code></pre> <p>Warning</p> <p>If the dashboard page fails to open, just click on the hyperlink in the console output.</p> <p>Note</p> <p>The <code>istioctl dashboard</code> command also blocks. Leave it running until you're finished using the dashboard, at which time  pressing Ctrl+C can interrupt the process and put you back at the terminal prompt.</p> <p>The Kiali dashboard displays.</p> <p>Customize the view as follows:</p> <ol> <li>Select the Graph section from the sidebar.</li> <li>Under Select Namespaces (at the top of the page), select the <code>default</code> namespace, the location where the application's pods are running.</li> <li>From the third \"pulldown\" menu, select App graph.</li> <li>From the Display \"pulldown\", toggle on Traffic Animation and Security.</li> <li>From the footer, toggle the legend so that it is visible.  Take a moment to familiarize yourself with the legend.</li> </ol> <p>Observe the visualization and note the following:</p> <ul> <li>We can see traffic coming in through the ingress gateway to the <code>web-frontend</code>, and the subsequent calls from the <code>web-frontend</code> to the <code>customers</code> service</li> <li>The lines connecting the services are green, indicating healthy requests</li> <li>The small lock icon on each edge in the graph indicates that the traffic is secured with mutual TLS</li> </ul> <p>Such visualizations are helpful with understanding the flow of requests in the mesh, and with diagnosis.</p> <p>Feel free to spend more time exploring Kiali.</p> <p>We will revisit Kiali in a later lab to visualize traffic shifting such as when performing a blue-green or canary deployment.</p>"},{"location":"dashboards/#kiali-cleanup","title":"Kiali Cleanup","text":"<p>Close the Kiali dashboard.  Interrupt the <code>istioctl dashboard kiali</code> command by pressing Ctrl+C.</p>"},{"location":"dashboards/#zipkin","title":"Zipkin","text":"<p>Launch the Zipkin dashboard:</p> <pre><code>istioctl dashboard zipkin\n</code></pre> <p>The Zipkin dashboard displays.</p> <ul> <li>Click on the red '+' button and select serviceName.</li> <li>Select the service named <code>web-frontend.default</code> and click on the Run Query button (lightblue) to the right.</li> </ul> <p>A number of query results will display.  Each row is expandable and will display more detail in terms of the services participating in that particular trace.</p> <ul> <li>Click the Show button to the right of one of the traces having four (4) spans.</li> </ul> <p>The resulting view shows spans that are part of the trace, and more importantly how much time was spent within each span.  Such information can help diagnose slow requests and pin-point where the latency lies.</p> <p>Distributed tracing also helps us make sense of the flow of requests in a microservice architecture.</p>"},{"location":"dashboards/#zipkin-cleanup","title":"Zipkin Cleanup","text":"<p>Close the Zipkin dashboard.  Interrupt the <code>istioctl dashboard zipkin</code> command with Ctrl+C.</p>"},{"location":"dashboards/#prometheus","title":"Prometheus","text":"<p>Prometheus works by periodically calling a metrics endpoint against each running service (this endpoint is termed the \"scrape\" endpoint).  Developers normally have to instrument their applications to expose such an endpoint and return metrics information in the format the Prometheus expects.</p> <p>With Istio, this is done automatically by the Envoy sidecar.</p>"},{"location":"dashboards/#observe-how-envoy-exposes-a-prometheus-scrape-endpoint","title":"Observe how Envoy exposes a Prometheus scrape endpoint","text":"<ol> <li> <p>Capture the customers pod name to a variable.</p> <pre><code>CUSTOMERS_POD=$(kubectl get pod -l app=customers -ojsonpath='{.items[0].metadata.name}')\n</code></pre> </li> <li> <p>Run the following command:</p> <pre><code>kubectl exec $CUSTOMERS_POD -- curl -s localhost:15020/stats/prometheus  | grep istio_requests\n</code></pre> <p>Why port 15020?</p> <p>See Ports used by Istio sidecar proxy.</p> <p>The list of metrics returned by the endpoint is rather lengthy, so we just peek at \"istio_requests\" metric.  The full response contains many more metrics.</p> </li> </ol>"},{"location":"dashboards/#access-the-dashboard","title":"Access the dashboard","text":"<ol> <li> <p>Start the prometheus dashboard</p> <pre><code>istioctl dashboard prometheus\n</code></pre> </li> <li> <p>In the search field enter the metric named <code>istio_requests_total</code>, and click the Execute button (on the right).</p> </li> <li> <p>Select the tab named Graph to obtain a graphical representation of this metric over time.</p> <p>Note that you are looking at requests across the entire mesh, i.e. this includes both requests to <code>web-frontend</code> and to <code>customers</code>.</p> </li> <li> <p>As an example of Prometheus' dimensional metrics capability, we can ask for total requests having a response code of 200:</p> <pre><code>istio_requests_total{response_code=\"200\"}\n</code></pre> </li> <li> <p>With respect to requests, it's more interesting to look at the rate of incoming requests over a time window.  Try:</p> <pre><code>rate(istio_requests_total[5m])\n</code></pre> </li> </ol> <p>There's much more to the Prometheus query language (this may be a good place to start).</p> <p>Grafana consumes these metrics to produce graphs on our behalf.</p> <ul> <li>Close the Prometheus dashboard and terminate the corresponding <code>istioctl dashboard</code> command.</li> </ul>"},{"location":"dashboards/#grafana","title":"Grafana","text":"<ol> <li> <p>Launch the Grafana dashboard</p> <pre><code>istioctl dashboard grafana\n</code></pre> </li> <li> <p>From the sidebar, select Dashboards \u2192 Browse</p> </li> <li>Click on the folder named Istio to reveal pre-designed Istio-specific Grafana dashboards</li> <li>Explore the Istio Mesh Dashboard.  Note the Global Request Volume and Global Success Rate.</li> <li>Explore the Istio Service Dashboard.  First select the service <code>web-frontend</code> and inspect its metrics, then switch to the <code>customers</code> service and review its dashboard.</li> <li>Explore the Istio Workload Dashboard.  Select the <code>web-frontend</code> workload.  Look at Outbound Services and note the outbound requests to the customers service.  Select the <code>customers</code> workload and note that it makes no Oubtound Services calls.</li> </ol> <p>Feel free to further explore these dashboards.</p>"},{"location":"dashboards/#cleanup","title":"Cleanup","text":"<ol> <li>Terminate the <code>istioctl dashboard</code> command (Ctrl+C)</li> <li>Likewise, terminate the <code>siege</code> command</li> </ol>"},{"location":"dashboards/#next","title":"Next","text":"<p>We turn our attention next to security features of a service mesh.</p>"},{"location":"environment/","title":"Lab environment","text":""},{"location":"environment/#options","title":"Options","text":"<ol> <li> <p>If you brought your own Kubernetes cluster:</p> <ul> <li> <p>Istio version 1.17.2 officially supports Kubernetes versions 1.23 - 1.26.  Feel free to consult the Istio support status of Istio releases page for more information.</p> </li> <li> <p>We recommend a 3-worker node cluster of machine type \"e2-standard-2\" or similar, though a smaller cluster will likely work just fine.</p> </li> </ul> <p>If you have your own public cloud account:</p> <ul> <li> <p>On GCP, the following command should provision a GKE cluster of adequate size for the workshop:</p> <pre><code>gcloud container clusters create my-istio-cluster \\\n--cluster-version latest \\\n--machine-type \"e2-standard-2\" \\\n--num-nodes \"3\" \\\n--network \"default\"\n</code></pre> </li> <li> <p>Feel free to provision a K8S cluster on any infrastructure of your choosing.</p> </li> </ul> </li> <li> <p>If you received Google credentials from the workshop instructors:</p> <ul> <li>A Kubernetes cluster has already been provisioned for you.</li> <li>Your instructor will demonstrate the process of accessing and configuring your environment, described below.</li> <li>The instructions below explain in detail how to access your account, select your project, and launch the cloud shell.</li> </ul> </li> <li> <p>Killercoda:  If you prefer to do away with having to setup your own Kubernetes environment, Killercoda offers a simple browser-based interactive environment.  The Istio 0 to 60 scenarios have been ported to Killercoda and can be launched from here.</p> <p>If you choose this option, please disregard this page's remaining instructions.</p> </li> <li> <p>Local:  Yet another option is to run a Kubernetes cluster on your local machine using Minikube, Kind, or similar tooling.  This option entails minimum resource (cpu and memory) requirements and you will need to ensure that ingress to loadbalancer-type services functions.  Here is a recipe for creating a local Kubernetes cluster with k3d:</p> <pre><code>k3d cluster create my-istio-cluster \\\n--api-port 6443 \\\n--k3s-arg \"--disable=traefik@server:0\" \\\n--port 80:80@loadbalancer\n</code></pre> </li> </ol> <p>Be sure to:</p> <ul> <li>Configure your <code>kubeconfig</code> file to point to your cluster.</li> <li>Follow the instructions at the bottom of this page to download the artifacts you will need for the upcoming labs.</li> </ul> <p>If you are bringing your own Kubernetes cluster, please skip ahead to the artifacts section at the bottom of this page.</p>"},{"location":"environment/#log-in-to-gcp","title":"Log in to GCP","text":"<ol> <li>Log in to GCP using credentials provided by your instructor.</li> <li>Agree to the terms</li> <li>You will be prompted to select your country, click \"Agree and continue\"</li> </ol>"},{"location":"environment/#select-your-project","title":"Select your project","text":"<p>Select the GCP project you have been assigned, as follows:</p> <ol> <li>Click the project selector \"pulldown\" menu from the top banner, which will open a popup dialog</li> <li>Make sure the Select from organization is set to tetratelabs.com</li> <li>Select the tab named All</li> <li>You will see your GCP project name (istio-0to60..) listed under the organization tetratelabs.com</li> <li>Select the project from the list</li> </ol> <p>Verify that your project is selected:</p> <ul> <li>If you look in the banner now, you will see your selected project displayed.</li> </ul>"},{"location":"environment/#launch-the-cloud-shell","title":"Launch the Cloud Shell","text":"<p>The Google Cloud Shell will serve as your terminal environment for these labs.</p> <ul> <li>Click the Activate cloud shell icon (top right); the icon looks like this: </li> <li>A dialog may pop up, click Continue</li> <li>Your cloud shell terminal should appear at the bottom of the screen</li> <li>Feel free to expand the size of the cloud shell, or even open it in a separate window (locate the icon button  in the terminal header, on the right)</li> </ul> <p>Warning</p> <p>Your connection to the Cloud Shell gets severed after a period of inactivity. Click on the Reconnect button when this happens.</p>"},{"location":"environment/#configure-cluster-access","title":"Configure cluster access","text":"<ol> <li> <p>Check that the <code>kubectl</code> CLI is installed</p> <pre><code>kubectl version --short\n</code></pre> </li> <li> <p>Generate a kubeconfig entry</p> With the user interfaceFrom the command line <ol> <li>Activate the top navigation menu (Menu icon on the top left hand side of the page)</li> <li>Locate and click on the product Kubernetes Engine (you may have to scroll down until you see it)</li> <li>Your pre-provisioned 3-node Kubernetes cluster should appear in the main view</li> <li>Click on that row's \"three dot\" menu and select the Connect option</li> <li>A dialog prompt will appear with instructions</li> <li>Copy the <code>gcloud</code> command shown and paste it in your cloud shell</li> </ol> <pre><code>gcloud container clusters get-credentials \\\n$(gcloud container clusters list --format=\"value(name)\") \\\n--zone $(gcloud container clusters list --format=\"value(location)\") \\\n--project $(gcloud config get-value project)\n</code></pre> <p>Click Authorize when prompted</p> <p>The console message will state that a kubeconfig entry [was] generated for [your project]</p> </li> <li> <p>Verify that your Kubernetes context is set for your cluster</p> <pre><code>kubectl config get-contexts\n</code></pre> </li> <li> <p>Run a token command such as <code>kubectl get node</code> or <code>kubectl get ns</code> to ensure that you can communicate with the Kubernetes API Server.</p> <pre><code>kubectl get ns\n</code></pre> </li> </ol> <p>Tip</p> <p>This workshop makes extensive use of the <code>kubectl</code> CLI.</p> <p>Consider configuring an alias to make typing a little easier.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; ~/.bashrc\nsource &lt;(kubectl completion bash)\nalias k=kubectl\ncomplete -F __start_kubectl k\nEOF\nsource ~/.bashrc\n</code></pre> <p>All instructions in subsequent labs assume you will be working from the Google Cloud Shell.</p>"},{"location":"environment/#artifacts","title":"Artifacts","text":"<p>The lab instructions reference Kubernetes yaml artifacts that you will need to apply to your cluster at specific points in time.</p> <p>You have the option of copying and pasting the yaml snippets directly from the lab instructions as you encounter them.</p> <p>Another option is to clone the GitHub repository for this workshop from the Cloud Shell.  You will find all yaml artifacts in the subdirectory named <code>artifacts</code>.</p> <pre><code>git clone https://github.com/tetratelabs/istio-0to60.git &amp;&amp; \\\nmv istio-0to60/artifacts . &amp;&amp; \\\nrm -rf istio-0to60\n</code></pre>"},{"location":"environment/#next","title":"Next","text":"<p>Now that we have access to our environment and to our Kubernetes cluster, we can proceed to install Istio.</p>"},{"location":"ingress/","title":"Ingress","text":"<p>The objective of this lab is to expose the <code>web-frontend</code> service to the internet.</p>"},{"location":"ingress/#the-ingress-gateway","title":"The Ingress gateway","text":"<p>When you installed Istio, in addition to deploying istiod to Kubernetes, the installation also provisioned an Ingress Gateway.</p> <p>View the corresponding Istio ingress gateway pod in the <code>istio-system</code> namespace.</p> <pre><code>kubectl get pod -n istio-system\n</code></pre> <p>A corresponding LoadBalancer type service was also created:</p> <pre><code>kubectl get svc -n istio-system\n</code></pre> <p>Make a note of the external IP address for the load balancer.</p> <p>Assign it to an environment variable.</p> <pre><code>GATEWAY_IP=$(kubectl get svc -n istio-system istio-ingressgateway -ojsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre> A small investment <p>When the cloud shell connection is severed, or when opening a new terminal tab, <code>$GATEWAY_IP</code> will no longer be in scope.</p> <p>Ensure <code>GATEWAY_IP</code> is set each time we start a new shell:</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; ~/.bashrc\nexport GATEWAY_IP=$(kubectl get svc -n istio-system istio-ingressgateway -ojsonpath='{.status.loadBalancer.ingress[0].ip}')\nEOF\n</code></pre> <p>In normal circumstances we associate this IP address with a hostname via DNS. For the sake of simplicity, in this workshop we will use the gateway public IP address directly.</p>"},{"location":"ingress/#configuring-ingress","title":"Configuring ingress","text":"<p>Configuring ingress with Istio is performed in two parts:</p> <ol> <li>Define a <code>Gateway</code> custom resource that governs the specific host, port, and protocol to expose</li> <li>Specify how requests should be routed with a <code>VirtualService</code> custom resource.</li> </ol>"},{"location":"ingress/#create-a-gateway-resource","title":"Create a Gateway resource","text":"<ol> <li> <p>Review the following Gateway specification.</p> <p>gateway.yaml</p> <pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\nname: frontend-gateway\nspec:\nselector:\nistio: ingressgateway\nservers:\n- port:\nnumber: 80\nname: http\nprotocol: HTTP\nhosts:\n- \"*\"\n</code></pre> <p>Above, we specify the HTTP protocol, port 80, and a wildcard (\"*\") host matcher which ensures that HTTP requests using the load balancer IP address <code>$GATEWAY_IP</code> will match.</p> <p>The selector istio: ingressgateway ensures that this gateway resource binds to the physical ingress gateway.</p> </li> <li> <p>Apply the gateway resource to your cluster.</p> <pre><code>kubectl apply -f gateway.yaml\n</code></pre> </li> <li> <p>Attempt an HTTP request in your browser to the gateway IP address.  It should return a 404 (not found).</p> </li> </ol>"},{"location":"ingress/#create-a-virtualservice-resource","title":"Create a VirtualService resource","text":"<ol> <li> <p>Review the following VirtualService specification.</p> web-frontend-virtualservice.yaml <pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: web-frontend\nspec:\nhosts:\n- \"*\"\ngateways:\n- frontend-gateway\nhttp:\n- route:\n- destination:\nhost: web-frontend.default.svc.cluster.local\nport:\nnumber: 80\n</code></pre> <p>Note how this specification references the name of the gateway (\"frontend-gateway\"), a matching host (\"*\"), and specifies a route for requests to be directed to the <code>web-frontend</code> service.</p> </li> <li> <p>Apply the virtual service resource to your cluster.</p> <pre><code>kubectl apply -f web-frontend-virtualservice.yaml\n</code></pre> </li> <li> <p>List virtual services in the default namespace.</p> <pre><code>kubectl get virtualservice\n</code></pre> <p>The output indicates that the virtual service named <code>web-frontend</code> is bound to the gateway, as well as any hostname that routes to the load balancer IP address.</p> </li> </ol> <p>Finally, verify that you can now access <code>web-frontend</code> from your web browser using the gateway IP address.</p>"},{"location":"ingress/#candidate-follow-on-exercises","title":"Candidate follow-on exercises","text":"<p>We will not explore ingress any further in this workshop.  Consider the following as independent exercises:</p> <ul> <li>Creating a DNS A record for the gateway IP, and narrowing down the scope of the gateway to only match that hostname.</li> <li>Configuring a TLS ingress gateway</li> </ul>"},{"location":"ingress/#next","title":"Next","text":"<p>The application is now running and exposed on the internet.</p> <p>In the next lab, we turn our attention to the observability features that are built in to Istio.</p>"},{"location":"install/","title":"Install Istio","text":"<p>In this lab you will install Istio.</p>"},{"location":"install/#download-istio","title":"Download Istio","text":"<ol> <li> <p>Run the following command from your home directory.</p> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.17.2 sh -\n</code></pre> </li> <li> <p>Navigate into the directory created by the above command.</p> <pre><code>cd istio-1.17.2\n</code></pre> </li> </ol>"},{"location":"install/#add-istioctl-to-your-path","title":"Add <code>istioctl</code> to your PATH","text":"<p>The <code>istioctl</code> CLI is located in the <code>bin/</code> subdirectory.</p> <p>Note</p> <p>Cloud Shell only preserves files located inside your home directory across sessions.</p> <p>This means that if you install a binary to a <code>PATH</code> such as <code>/usr/local/bin</code>, after your session times out that file will no longer be there!</p> <p>As a workaround, you will add <code>${HOME}/bin</code> to your <code>PATH</code> and place the binary there.</p> <ol> <li> <p>Create a <code>bin</code> subdirectory in your home directory:</p> <pre><code>mkdir ~/bin\n</code></pre> </li> <li> <p>Copy the CLI to that subdirectory:</p> <pre><code>cp ./bin/istioctl ~/bin\n</code></pre> </li> <li> <p>Add your home <code>bin</code> subdirectory to your <code>PATH</code></p> <pre><code>cat &lt;&lt; EOF &gt;&gt; ~/.bashrc\nexport PATH=\"~/bin:\\$PATH\"\nEOF\n</code></pre> <p>And then:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> <p>Verify that <code>istioctl</code> is installed with:</p> <pre><code>istioctl version\n</code></pre> <p>The output should indicate that the version is 1.17.2.</p> <p>With the CLI installed, proceed to install Istio to Kubernetes.</p>"},{"location":"install/#pre-check","title":"Pre-check","text":"<p>The <code>istioctl</code> CLI provides a convenient <code>precheck</code> command that can be used to \"inspect a Kubernetes cluster for Istio install and upgrade requirements.\"</p> <p>To verify whether it is safe to install Istio on your Kubernetes cluster, run:</p> <pre><code>istioctl x precheck\n</code></pre> <p>Make sure that the output of the above command returns a green \"checkmark\" stating that no issues were found when checking the cluster.</p>"},{"location":"install/#install-istio_1","title":"Install Istio","text":"<ol> <li> <p>Istio can be installed directly with the CLI:</p> <pre><code>istioctl install\n</code></pre> </li> <li> <p>When prompted, enter <code>y</code> to proceed to install Istio.</p> </li> </ol> <p>Take a moment to learn more about Istio installation profiles.</p>"},{"location":"install/#verify-that-istio-is-installed","title":"Verify that Istio is installed","text":"<p>Post-installation, Istio provides the command <code>verify-install</code>: it runs a series of checks to ensure that the installation was successful and complete.</p> <p>Go ahead and run it:</p> <pre><code>istioctl verify-install\n</code></pre> <p>Inspect the output and confirm that the it states that \"\u2714 Istio is installed and verified successfully.\"</p> <p>Keep probing:</p> <ol> <li> <p>List Kubernetes namespaces and note the new namespace <code>istio-system</code></p> <pre><code>kubectl get ns\n</code></pre> </li> <li> <p>Verify that the <code>istiod</code> controller pod is running in that namespace</p> <pre><code>kubectl get pod -n istio-system\n</code></pre> </li> <li> <p>Re-run <code>istioctl version</code>.  The output should include a control plane version, indicating that Istio is indeed present in the cluster.</p> </li> </ol>"},{"location":"install/#next","title":"Next","text":"<p>With Istio installed, we are ready to deploy an application to the mesh.</p>"},{"location":"security/","title":"Security","text":"<p>In this lab we explore some of the security features of the Istio service mesh.</p>"},{"location":"security/#mutual-tls","title":"Mutual TLS","text":"<p>By default, Istio is configured such that when a service is deployed onto the mesh, it will take advantage of mutual TLS:</p> <ul> <li>the service is given an identity as a function of its associated service account and namespace</li> <li>an x.509 certificate is issued to the workload (and regularly rotated) and used to identify the workload in calls to other services</li> </ul> <p>In the observability lab, we looked at the Kiali dashboard and noted the lock icons indicating that traffic was secured with mTLS.</p>"},{"location":"security/#can-a-workload-receive-plain-text-requests","title":"Can a workload receive plain-text requests?","text":"<p>We can test whether a mesh workload, such as the customers service, will allow a plain-text request as follows:</p> <ol> <li> <p>Create a separate namespace that is not configured with automatic injection.</p> <pre><code>kubectl create ns otherns\n</code></pre> </li> <li> <p>Deploy <code>sleep</code> to that namespace</p> <pre><code>kubectl apply -f sleep.yaml -n otherns\n</code></pre> </li> <li> <p>Verify that the sleep pod has no sidecars:</p> <pre><code>kubectl get pod -n otherns\n</code></pre> </li> <li> <p>Call the customer service from that pod:</p> <pre><code>SLEEP_POD=$(kubectl get pod -l app=sleep -n otherns -ojsonpath='{.items[0].metadata.name}')\nkubectl exec -n otherns $SLEEP_POD -- curl -s customers.default\n</code></pre> </li> </ol> <p>The output should look like a list of customers in JSON format.</p> <p>We conclude that Istio is configured by default to allow plain-text request. This is called permissive mode and is specifically designed to allow services that have not yet fully onboarded onto the mesh to participate.</p>"},{"location":"security/#enable-strict-mode","title":"Enable strict mode","text":"<p>Istio provides the <code>PeerAuthentication</code> custom resource to define peer authentication policy.</p> <ol> <li> <p>Apply the following peer authentication policy.</p> mtls-strict.yaml <pre><code>---\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\nname: default\nnamespace: default\nspec:\nmtls:\nmode: STRICT\n</code></pre> <p>Info</p> <p>Strict mtls can be enabled globally by setting the namespace to the name of the Istio root namespace, which by default is <code>istio-system</code></p> </li> <li> <p>Verify that the peer authentication has been applied.</p> <pre><code>kubectl get peerauthentication\n</code></pre> </li> </ol>"},{"location":"security/#verify-that-plain-text-requests-are-no-longer-permitted","title":"Verify that plain-text requests are no longer permitted","text":"<pre><code>kubectl exec -n otherns $SLEEP_POD -- curl customers.default\n</code></pre> <p>The console output should indicate that the connection was reset by peer.</p>"},{"location":"security/#security-in-depth","title":"Security in depth","text":"<p>Another important layer of security is to define an authorization policy, in which we allow only specific services to communicate with other services.</p> <p>At the moment, any container can, for example, call the customers service or the web-frontend service.</p> <ol> <li> <p>Capture the name of the sleep pod running in the default namespace</p> <pre><code>SLEEP_POD=$(kubectl get pod -l app=sleep -ojsonpath='{.items[0].metadata.name}')\n</code></pre> </li> <li> <p>Call the <code>customers</code> service.</p> <pre><code>kubectl exec $SLEEP_POD -- curl -s customers\n</code></pre> </li> <li> <p>Call the <code>web-frontend</code> service.</p> <pre><code>kubectl exec $SLEEP_POD -- curl -s web-frontend | head\n</code></pre> </li> </ol> <p>Both calls succeed.</p> <p>We wish to apply a policy in which only <code>web-frontend</code> is allowed to call <code>customers</code>, and only the ingress gateway can call <code>web-frontend</code>.</p> <p>Study the below authorization policy.</p> <p>authz-policy-customers.yaml</p> <pre><code>---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\nname: allowed-customers-clients\nnamespace: default\nspec:\nselector:\nmatchLabels:\napp: customers\naction: ALLOW\nrules:\n- from:\n- source:\nprincipals: [\"cluster.local/ns/default/sa/web-frontend\"]\n</code></pre> <ul> <li>The <code>selector</code> section specifies that the policy applies to the <code>customers</code> service.</li> <li>Note how the rules have a \"from: source: \" section indicating who is allowed in.</li> <li>The nomenclature for the value of the <code>principals</code> field comes from the spiffe standard.  Note how it captures the service account name and namespace associated with the <code>web-frontend</code> service.  This identify is associated with the x.509 certificate used by each service when making secure mtls calls to one another.</li> </ul> <p>Tasks:</p> <ul> <li> Apply the policy to your cluster.</li> <li> Verify that you are no longer able to reach the <code>customers</code> pod from the <code>sleep</code> pod</li> </ul>"},{"location":"security/#challenge","title":"Challenge","text":"<p>Can you come up with a similar authorization policy for <code>web-frontend</code>?</p> <ul> <li>Use a copy of the <code>customers</code> authorization policy as a starting point</li> <li>Give the resource an apt name</li> <li>Revise the selector to match the <code>web-frontend</code> service</li> <li>Revise the rule to match the principal of the ingress gateway</li> </ul> <p>Hint</p> <p>The ingress gateway has its own identity.</p> <p>Here is a command which can help you find the name of the service account associated with its identity:</p> <pre><code>kubectl get pod -n istio-system -l app=istio-ingressgateway -o yaml | grep serviceAccountName\n</code></pre> <p>Use this service account name together with the namespace that the ingress gateway is running in to specify the value for the <code>principals</code> field.</p>"},{"location":"security/#test-it","title":"Test it","text":"<p>Don't forget to verify that the policy is enforced.</p> <ul> <li>Call both services again from the sleep pod and ensure communication is no longer allowed.</li> <li>The console output should contain the message RBAC: access denied.</li> </ul>"},{"location":"security/#next","title":"Next","text":"<p>In the next lab we show how to use Istio's traffic management features to upgrade the customers service with zero downtime.</p>"},{"location":"sidecar-injection/","title":"Sidecar injection","text":"<p>This lab explores sidecar injection in Istio.</p>"},{"location":"sidecar-injection/#preface","title":"Preface","text":"<p>Istio provides both a manual and an automatic mechanism for injecting sidecars alongside workloads.</p> <p>In this lab you will use the manual method, because it provides the opportunity to inspect the transformed deployment manifest even before applying it to a target Kubernetes cluster.</p> <p>You will learn about automatic sidecar injection in the next lab.</p>"},{"location":"sidecar-injection/#generate-a-pod-spec","title":"Generate a Pod spec","text":"<p>The <code>kubectl</code> command's <code>dry-run</code> flag provides a simple way to generate and capture a simple pod specification.</p> <p>Generate a Pod spec for a simple web server, as follows:</p> <pre><code>kubectl run mywebserver --image nginx \\\n--dry-run=client -oyaml &gt; nginx-pod.yaml\n</code></pre> <p>Inspect the contents of the generated file.  Here it is below, slightly cleaned up:</p> nginx-pod.yaml<pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\nlabels:\nrun: mywebserver\nname: mywebserver\nspec:\ncontainers:\n- name: mywebserver\nimage: nginx\n</code></pre> <p>The main thing to note at this point is that this Pod spec consists of a single container using the image <code>nginx</code>.</p>"},{"location":"sidecar-injection/#transform-the-pod-spec","title":"Transform the Pod spec","text":"<p>The <code>istioctl</code> command provides the convenient <code>kube-inject</code> subcommand, that can transform such a specification into one that includes the necessary sidecar.</p> <ol> <li> <p>Learn the <code>kube-inject</code> command's usage:</p> <pre><code>istioctl kube-inject --help\n</code></pre> </li> <li> <p>Use the command to generate and capture the full sidecar-injected manifest to a new file named <code>transformed.yaml</code>.</p> Show me how <pre><code>istioctl kube-inject --filename ./nginx-pod.yaml &gt; transformed.yaml\n</code></pre> </li> </ol>"},{"location":"sidecar-injection/#study-the-sidecar-container-specification","title":"Study the sidecar container specification","text":"<p>The modified Pod specification now includes a second container.</p> <p>Here is the salient part:</p> <pre><code>  - name: istio-proxy\nimage: docker.io/istio/proxyv2:1.17.2\nargs:\n- proxy\n- sidecar\n- --domain\n- $(POD_NAMESPACE).svc.cluster.local\n- --proxyLogLevel=warning\n- --proxyComponentLogLevel=misc:error\n- --log_output_level=default:info\n- --concurrency\n- \"2\"\nenv:\n- ...\n</code></pre> <p>The container name is <code>istio-proxy</code> and the docker image is <code>istio/proxyv2</code>.</p> What command is actually run? <p>To find out what command actually runs inside that container, we can inspect the docker container specification and view the Entrypoint field:</p> <pre><code>docker pull docker.io/istio/proxyv2:1.17.2\ndocker inspect istio/proxyv2:1.17.2 | grep Entrypoint -A 2\n</code></pre> <p>Here is the output:</p> <pre><code>\"Entrypoint\": [\n\"/usr/local/bin/pilot-agent\"\n],\n</code></pre> <p>We learn that the name of the command is <code>pilot-agent</code>.</p> <p>By extracting the arguments from the yaml, we can reconstitute the full command executed inside the sidecar container:</p> <pre><code>pilot-agent proxy sidecar \\\n--domain $(POD_NAMESPACE).svc.cluster.local \\\n--proxyLogLevel=warning \\\n--proxyComponentLogLevel=misc:error \\\n--log_output_level=default:info \\\n--concurrency \"2\"\n</code></pre>"},{"location":"sidecar-injection/#apply-the-manifest","title":"Apply the manifest","text":"<ol> <li> <p>Deploy the transformed manifest to Kubernetes:</p> <pre><code>kubectl apply -f transformed.yaml\n</code></pre> </li> <li> <p>List pods in the <code>default</code> namespace</p> <pre><code>kubectl get pod\n</code></pre> <p>Once the pod reaches <code>Running</code> state, note the <code>READY</code> column in the output displays 2 out of 2 containers:</p> <pre><code>NAME          READY   STATUS    RESTARTS   AGE\nmywebserver   2/2     Running   0          36s\n</code></pre> </li> </ol>"},{"location":"sidecar-injection/#study-the-running-processes","title":"Study the running processes","text":"<p>Run the <code>ps</code> command from inside the sidecar container, like so:</p> <pre><code>kubectl exec mywebserver -c istio-proxy -- ps -ef\n</code></pre> <p>Here is the output, slightly cleaned up, showing both the <code>pilot-agent</code> process, and the <code>envoy</code> process that it bootstrapped:</p> <pre><code>PID  PPID CMD\n  1     0 /usr/local/bin/pilot-agent proxy sidecar --domain ...\n 16     1 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json ...\n</code></pre> <p>We can learn more about the <code>pilot-agent</code> command by running <code>pilot-agent --help</code> from inside the sidecar container:</p> <pre><code>kubectl exec mywebserver -c istio-proxy -- pilot-agent --help\n</code></pre>"},{"location":"sidecar-injection/#study-the-initcontainers-specification","title":"Study the <code>initContainers</code> specification","text":"<p>Besides injecting a sidecar container, the transformation operation also adds an initContainers section.</p> <p>Here is the relevant section:</p> <pre><code>  initContainers:\n- name: istio-init\nimage: docker.io/istio/proxyv2:1.17.2\nargs:\n- istio-iptables\n- -p\n- \"15001\"\n- -z\n- \"15006\"\n- -u\n- \"1337\"\n- -m\n- REDIRECT\n- -i\n- '*'\n- -x\n- \"\"\n- -b\n- '*'\n- -d\n- 15090,15021,15020\n- --log_output_level=default:info\n</code></pre> <p>The \"initContainer\" uses the same image as the sidecar container: <code>istio/proxyv2</code>.  The difference lies in the command that is run when the Pod initializes.</p> <p>Here is the reconstituted command with long-form versions of each option, to clarify the instruction:</p> <pre><code>pilot-agent istio-iptables \\\n--envoy-port \"15001\" \\\n--inbound-capture-port \"15006\" \\\n--proxy-uid \"1337\" \\\n--istio-inbound-interception-mode REDIRECT \\\n--istio-service-cidr '*' \\\n--istio-service-exclude-cidr \"\" \\\n--istio-inbound-ports '*' \\\n--istio-local-exclude-ports 15090,15021,15020 \\\n--log_output_level=default:info\n</code></pre> Tip <p>For a full description of the <code>istio-iptables</code> subcommand and its options, run:</p> <pre><code>kubectl exec mywebserver -c istio-proxy -- pilot-agent istio-iptables --help\n</code></pre> <p>The gist of the command is that, through <code>iptables</code> rules, the routing of network packets inside the Pod is reconfigured to give Envoy the chance to intercept and proxy inbound and outbound traffic.</p> <p>We need not concern ourselves with the specific port numbers, exclusions, and other low-level details at this time.</p> <p>The lesson of this exercise is to learn how to get at these details.</p>"},{"location":"sidecar-injection/#going-forward","title":"Going forward..","text":"<p>The above process of transforming a deployment manifest on its way to the Kube API Server is streamlined when using automatic sidecar injection.</p> <p>The next lab will walk you through how automatic sidecar injection is accomplished.</p> <p>From here on, we will use automatic sidecar injection when deploying workloads to the mesh.</p>"},{"location":"sidecar-injection/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete -f transformed.yaml\n</code></pre>"},{"location":"summary/","title":"Congratulations","text":"<p>Well-done on making it all the way to the end of the Istio 0 to 60 workshop!</p> <p>In this workshop, you have covered a lot of ground!</p> <p>Let's summarize.  You have:</p> <ul> <li> Installed Istio</li> <li> Deployed an application</li> <li> Exposed the application to the internet (Ingress)</li> <li> Deployed and studied observability addons including Kiali, Zipkin, Prometheus, and Grafana</li> <li> Studied facets of service mesh security including mutual tls and authorization policies</li> <li> Performed a traffic shifting exercise</li> </ul> <p>Istio has many more features whose scope is beyond the 0 to 60 workshop, including but not limited to:</p> <ul> <li> Failure injection</li> <li> Circuit breakers</li> <li> Extensibility with WASM</li> <li> Egress gateways</li> <li> Onboarding VM Workloads</li> <li> Istio deployment models</li> </ul> <p>We encourage you to dig deeper into the Istio docs yourself.</p> <p>You might also be interested in the free courses offered at the Tetrate Academy, including Istio Fundamentals, and Envoy Fundamentals.</p> <p>Finally, if you're interested in certification, check out the Istio Certified Administrator exam.</p> <p>Thanks!</p>"},{"location":"the-app/","title":"The application","text":"<p>In this lab you will deploy an application to your mesh.</p> <ul> <li> <p>The application consists of two microservices, <code>web-frontend</code> and <code>customers</code>.</p> Info <p>The official Istio docs canonical example is the BookInfo application.</p> <p>For this workshop we felt that an application involving fewer microservices would be more clear.</p> </li> <li> <p>The <code>customers</code> service exposes a REST endpoint that returns a list of customers in JSON format.  The <code>web-frontend</code> calls <code>customers</code> to retrieve the list, which it uses to render to HTML.</p> </li> <li> <p>The respective Docker images for these services have already been built and pushed to a Docker registry.</p> </li> <li> <p>You will deploy the application to the <code>default</code> Kubernetes namespace.</p> </li> </ul> <p>But before proceeding, we must enable sidecar injection.</p>"},{"location":"the-app/#enable-automatic-sidecar-injection","title":"Enable automatic sidecar injection","text":"<p>There are two options for sidecar injection: automatic and manual.</p> <p>In this lab we will use automatic injection, which involves labeling the namespace where the pods are to reside.</p> <ol> <li> <p>Label the default namespace</p> <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre> </li> <li> <p>Verify that the label has been applied:</p> <pre><code>kubectl get ns -Listio-injection\n</code></pre> </li> </ol>"},{"location":"the-app/#deploy-the-application","title":"Deploy the application","text":"<ol> <li> <p>Study the two Kubernetes yaml files: <code>web-frontend.yaml</code> and <code>customers.yaml</code>.</p> web-frontend.yaml <pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: web-frontend\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: web-frontend\nlabels:\napp: web-frontend\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: web-frontend\ntemplate:\nmetadata:\nlabels:\napp: web-frontend\nversion: v1\nspec:\nserviceAccountName: web-frontend\ncontainers:\n- image: gcr.io/tetratelabs/web-frontend:1.0.0\nimagePullPolicy: Always\nname: web\nports:\n- containerPort: 8080\nenv:\n- name: CUSTOMER_SERVICE_URL\nvalue: \"http://customers.default.svc.cluster.local\"\n---\nkind: Service\napiVersion: v1\nmetadata:\nname: web-frontend\nlabels:\napp: web-frontend\nspec:\nselector:\napp: web-frontend\nports:\n- port: 80\nname: http\ntargetPort: 8080\n</code></pre> customers.yaml <pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: customers\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: customers-v1\nlabels:\napp: customers\nversion: v1\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: customers\nversion: v1\ntemplate:\nmetadata:\nlabels:\napp: customers\nversion: v1\nspec:\nserviceAccountName: customers\ncontainers:\n- image: gcr.io/tetratelabs/customers:1.0.0\nimagePullPolicy: Always\nname: svc\nports:\n- containerPort: 3000\n---\nkind: Service\napiVersion: v1\nmetadata:\nname: customers\nlabels:\napp: customers\nspec:\nselector:\napp: customers\nports:\n- port: 80\nname: http\ntargetPort: 3000\n</code></pre> <p>Each file defines its corresponding deployment, service account, and ClusterIP service.</p> </li> <li> <p>Apply the two files to your Kubernetes cluster.</p> <pre><code>kubectl apply -f customers.yaml\n</code></pre> <pre><code>kubectl apply -f web-frontend.yaml\n</code></pre> </li> </ol> <p>Confirm that:</p> <ul> <li>Two pods are running, one for each service</li> <li> <p>Each pod consists of two containers, the one running the service image, plus the Envoy sidecar</p> <pre><code>kubectl get pod\n</code></pre> </li> </ul> <p>How did each pod end up with two containers?</p> <p>Istio installs a Kubernetes object known as a mutating webhook admission controller: logic that intercepts Kubernetes object creation requests and that has the permission to alter (mutate) what ends up stored in etcd (the pod spec).</p> <p>You can list the mutating webhooks in your Kubernetes cluster and confirm that the sidecar injector is present.</p> <pre><code>kubectl get mutatingwebhookconfigurations\n</code></pre>"},{"location":"the-app/#verify-access-to-each-service","title":"Verify access to each service","text":"<p>We wish to deploy a pod that runs a <code>curl</code> image so we can verify that each service is reachable from within the cluster. The Istio distribution provides a sample app called <code>sleep</code> that will serve this purpose.</p> <ol> <li> <p>Deploy <code>sleep</code> to the default namespace.</p> sleep.yaml <pre><code># Copyright Istio Authors\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n##################################################################################################\n# Sleep service\n##################################################################################################\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: sleep\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: sleep\nlabels:\napp: sleep\nservice: sleep\nspec:\nports:\n- port: 80\nname: http\nselector:\napp: sleep\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: sleep\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: sleep\ntemplate:\nmetadata:\nlabels:\napp: sleep\nspec:\nterminationGracePeriodSeconds: 0\nserviceAccountName: sleep\ncontainers:\n- name: sleep\nimage: curlimages/curl\ncommand: [\"/bin/sleep\", \"3650d\"]\nimagePullPolicy: IfNotPresent\nvolumeMounts:\n- mountPath: /etc/sleep/tls\nname: secret-volume\nvolumes:\n- name: secret-volume\nsecret:\nsecretName: sleep-secret\noptional: true\n---\n</code></pre> <pre><code>kubectl apply -f sleep.yaml\n</code></pre> </li> <li> <p>Capture the name of the sleep pod to an environment variable</p> <pre><code>SLEEP_POD=$(kubectl get pod -l app=sleep -ojsonpath='{.items[0].metadata.name}')\n</code></pre> </li> <li> <p>Use the <code>kubectl exec</code> command to call the <code>customers</code> service.</p> <pre><code>kubectl exec $SLEEP_POD -- curl -s customers\n</code></pre> <p>The console output should show a list of customers in JSON format.</p> </li> <li> <p>Call the <code>web-frontend</code> service</p> <pre><code>kubectl exec $SLEEP_POD -- curl -s web-frontend | head\n</code></pre> <p>The console output should show the start of an HTML page listing customers in an HTML table.</p> </li> </ol>"},{"location":"the-app/#next","title":"Next","text":"<p>In the next lab, we expose the <code>web-frontend</code> using an Istio Ingress Gateway.</p> <p>This will allow us to access this application on the web.</p>"},{"location":"traffic-shifting/","title":"Traffic shifting","text":"<p>Version 2 of the customers service has been developed, and it's time to deploy it to production. Whereas version 1 returned a list of customer names, version 2 also includes each customer's city.</p>"},{"location":"traffic-shifting/#deploying-customers-v2","title":"Deploying customers, v2","text":"<p>We wish to deploy the new service but aren't yet ready to direct traffic to it.</p> <p>It would be prudent to separate the task of deploying the new service from the task of directing traffic to it.</p>"},{"location":"traffic-shifting/#labels","title":"Labels","text":"<p>The customers service is labeled with <code>app=customers</code>.</p> <p>Verify this with:</p> <pre><code>kubectl get pod -Lapp,version\n</code></pre> <p>Note the selector on the customers service:</p> <pre><code>kubectl get svc customers -o wide\n</code></pre> <p>If we were to just deploy v2, the selector would match both versions.</p>"},{"location":"traffic-shifting/#destinationrules","title":"DestinationRules","text":"<p>We can inform Istio that two distinct subsets of the <code>customers</code> service exist, and we can use the <code>version</code> label as the discriminator.</p> customers-destinationrule.yaml<pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\nname: customers\nspec:\nhost: customers.default.svc.cluster.local\nsubsets:\n- name: v1\nlabels:\nversion: v1\n- name: v2\nlabels:\nversion: v2\n</code></pre> <ol> <li> <p>Apply the above destination rule to the cluster.</p> </li> <li> <p>Verify that it's been applied.</p> <pre><code>kubectl get destinationrule\n</code></pre> </li> </ol>"},{"location":"traffic-shifting/#virtualservices","title":"VirtualServices","text":"<p>Armed with two distinct destinations, the <code>VirtualService</code> custom resource allows us to define a routing rule that sends all traffic to the v1 subset.</p> customers-virtualservice.yaml<pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: customers\nspec:\nhosts:\n- customers.default.svc.cluster.local\nhttp:\n- route:\n- destination:\nhost: customers.default.svc.cluster.local\nsubset: v1\n</code></pre> <p>Above, note how the route specifies subset v1.</p> <ol> <li> <p>Apply the virtual service to the cluster.</p> </li> <li> <p>Verify that it's been applied.</p> <pre><code>kubectl get virtualservice </code></pre> </li> </ol>"},{"location":"traffic-shifting/#finally-deploy-customers-v2","title":"Finally deploy customers, v2","text":"<p>Apply the following Kubernetes deployment to the cluster.</p> customers-v2.yaml <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: customers-v2\nlabels:\napp: customers\nversion: v2\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: customers\nversion: v2\ntemplate:\nmetadata:\nlabels:\napp: customers\nversion: v2\nspec:\nserviceAccountName: customers\ncontainers:\n- image: gcr.io/tetratelabs/customers:2.0.0\nimagePullPolicy: Always\nname: svc\nports:\n- containerPort: 3000\n</code></pre>"},{"location":"traffic-shifting/#check-that-traffic-routes-strictly-to-v1","title":"Check that traffic routes strictly to v1","text":"<ol> <li> <p>Generate some traffic.</p> <pre><code>siege --delay=3 --concurrent=3 --time=20M http://$GATEWAY_IP/\n</code></pre> </li> <li> <p>Open a separate terminal and launch the Kiali dashboard</p> <pre><code>istioctl dashboard kiali\n</code></pre> </li> </ol> <p>Take a look at the graph, and select the <code>default</code> namespace.</p> <p>The graph should show all traffic going to v1.</p>"},{"location":"traffic-shifting/#route-to-customers-v2","title":"Route to customers, v2","text":"<p>We wish to proceed with caution.  Before customers can see version 2, we want to make sure that the service functions properly.</p>"},{"location":"traffic-shifting/#expose-debug-traffic-to-v2","title":"Expose \"debug\" traffic to v2","text":"<p>Review this proposed updated routing specification.</p> customers-vs-debug.yaml<pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: customers\nspec:\nhosts:\n- customers.default.svc.cluster.local\nhttp:\n- match:\n- headers:\nuser-agent:\nexact: debug\nroute:\n- destination:\nhost: customers.default.svc.cluster.local\nsubset: v2\n- route:\n- destination:\nhost: customers.default.svc.cluster.local\nsubset: v1\n</code></pre> <p>We are telling Istio to check an HTTP header:  if the <code>user-agent</code> is set to <code>debug</code>, route to v2, otherwise route to v1.</p> <p>Open a new terminal and apply the above resource to the cluster; it will overwrite the currently defined virtualservice as both yamls use the same resource name.</p> <pre><code>kubectl apply -f customers-vs-debug.yaml\n</code></pre>"},{"location":"traffic-shifting/#test-it","title":"Test it","text":"<p>Open a browser and visit the application.</p> If you need it <pre><code>GATEWAY_IP=$(kubectl get svc -n istio-system istio-ingressgateway -ojsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre> <p>We can tell v1 and v2 apart in that v2 displays not only customer names but also their city (in two columns).</p> <p>If you're using Chrome or Firefox, you can customize the <code>user-agent</code> header as follows:</p> <ol> <li>Open the browser's developer tools</li> <li>Open the \"three dots\" menu, and select More tools \u2192 Network conditions</li> <li>The network conditions panel will open</li> <li>Under User agent, uncheck Use browser default</li> <li>Select Custom... and in the text field enter <code>debug</code></li> </ol> <p>Refresh the page; traffic should be directed to v2.</p> <p>Tip</p> <p>If you refresh the page a good dozen times and then wait ~15-30 seconds, you should see some of that v2 traffic in Kiali.</p>"},{"location":"traffic-shifting/#canary","title":"Canary","text":"<p>Well, v2 looks good; we decide to expose the new version to the public, but we're still prudent.</p> <p>Start by siphoning 10% of traffic over to v2.</p> customers-vs-canary.yaml<pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: customers\nspec:\nhosts:\n- customers.default.svc.cluster.local\nhttp:\n- route:\n- destination:\nhost: customers.default.svc.cluster.local\nsubset: v2\nweight: 10\n- destination:\nhost: customers.default.svc.cluster.local\nsubset: v1\nweight: 90\n</code></pre> <p>Above, note the <code>weight</code> field specifying 10 percent of traffic to v2. Kiali should now show traffic going to both v1 and v2.</p> <ul> <li>Apply the above resource.</li> <li>In your browser:  undo the user agent customization and refresh the page a bunch of times.</li> </ul> <p>Most of the requests still go to v1, but some are directed to v2.</p>"},{"location":"traffic-shifting/#check-grafana","title":"Check Grafana","text":"<p>Before we open the floodgates, we wish to determine how v2 is fairing.</p> <pre><code>istioctl dashboard grafana\n</code></pre> <p>In Grafana, visit the Istio Workload Dashboard and specifically look at the customers v2 workload. Look at the request rate and the incoming success rate, also the latencies.</p> <p>If all looks good, up the percentage from 90/10 to, say 50/50.</p> <p>Watch the request volume change (you may need to click on the \"refresh dashboard\" button in the upper right-hand corner).</p> <p>Finally, switch all traffic over to v2.</p> customers-virtualservice-final.yaml<pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: customers\nspec:\nhosts:\n- customers.default.svc.cluster.local\nhttp:\n- route:\n- destination:\nhost: customers.default.svc.cluster.local\nsubset: v2\n</code></pre> <p>After you apply the above yaml, go to your browser and make sure all requests land on v2 (2-column output). Within a minute or so, the Kiali dashboard should also reflect the fact that all traffic is going to the customers v2 service.</p> <p>Though it no longer receives any traffic, we decide to leave v1 running a while longer before retiring it.</p>"}]}